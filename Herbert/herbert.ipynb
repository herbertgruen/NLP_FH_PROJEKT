{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspriation: [Quelle](https://nzlul.medium.com/the-classification-of-text-messages-using-lstm-bi-lstm-and-gru-f79b207f90ad)<br>\n",
    "Daten: [Quelle](https://github.com/kitsamho/songlyrics_univeral_sentence_encoder/tree/master/Lyric_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#%conda install pip\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Modeling\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "######## SEED setzen\n",
    "import random\n",
    "# Random Seed setzen:\n",
    "random.seed(49)\n",
    "np.random.seed(49)\n",
    "SEED = 49\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('lyrics.csv', encoding='ISO-8859-1')\n",
    "df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lyrics (Data) und Genere (Label) haben keinen Nullwert. Song (-Titel) fehelen 6 aber dass ist egal die können ignoriert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **lyric_count**: Mit der split-Funktion wird die Lyric in ihre einzelnen Wörter (auch Kommas?) geteilt und dann mit len-Funktion angezeigt. Sprich die Anzahl an Wörtern im Songtext.\n",
    "- **lyric_count_norm**: Dieses Feature entsteht, indem jeder Eintrag in der Spalte lyric_count mit dem max. aus lyric_count dividiert wird. So entsteht eine normalisierter Wert von lyric_count. Daraufhin wird dieser Wert der sich zwischen 1 und 0 befindet mit dem Wert 55 multipliziert. So entsteht ein neuer Wert in der Spalte lyric_count_norm. Der Wert wird sozusagen wieder hochskaliert.\n",
    "\n",
    "- [Link zur Analyse der Daten](https://www.notion.so/Notizen-Ivan-10-04-2024-8ba4890572db452a91ded46a5a276060)\n",
    "\n",
    "Meines erachten nichts relevantes ablesbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion zum Plotten der Modell History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot the history of a model\n",
    "def plot_history(history):\n",
    "    print(history.history.keys())\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(epochs, history.history['accuracy'], 'o-', label='Training accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'o-', label='Validation accuracy')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(epochs)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss, start from second epoch, cause the first is always too high\n",
    "    plt.plot(epochs[1:], history.history['loss'][1:], 'o-', label='Training loss')\n",
    "    plt.plot(epochs[1:], history.history['val_loss'][1:], 'o-', label='Validation loss')\n",
    "    plt.title('Model loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(epochs[1:])  # Adjust x-axis to start from second epoch\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL und Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(df.genre)\n",
    "plt.title('The distribution of genres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genres sehen erst Mal relativ gleichmäßig verteilt aus. Eventuell Später alle auf die ANzahl vom niedrigsten (Hip-Hop) kürzen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellen des numerischen Labels aus den Genres: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = label_encoder.fit_transform(df['genre'])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing der Daten (lyrics):\n",
    "- Lowercasing\n",
    "- Entfernung von Zahlen oder Umwandlung in Text\n",
    "- Entfernung von Satzzeichen\n",
    "- Entfernung von Stoppwörtern\n",
    "- Entfernung von Leerzeichen\n",
    "- Entfernung von dopplungen: \"yeah, yeah, yeah\" -> \"yeah\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sicherstellen, dass NLTK-Ressourcen heruntergeladen sind\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Funktion zur Entfernung von Wiederholungen (z.B. \"yeah yeah yeah\" -> \"yeah\")\n",
    "def remove_repetitions(text):\n",
    "    # Aufteilen des Texts in Wörter\n",
    "    words = text.split()\n",
    "    # Erstellung eines neuen Texts ohne wiederholte Wörter\n",
    "    new_text = ' '.join(sorted(set(words), key=words.index))\n",
    "    return new_text\n",
    "\n",
    "# Funktion zur Bereinigung des Lyrics-Textes\n",
    "def clean_lyrics(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Entfernung von Zahlen\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Entfernung von Satzzeichen\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Entfernung von mehrfachen Leerzeichen\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Entfernung von Stoppwörtern\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Entfernung von Wiederholungen wie \"yeah yeah yeah\"\n",
    "    text = remove_repetitions(text)\n",
    "    return text\n",
    "\n",
    "# Anwendung der Bereinigungsfunktion auf die Spalte 'lyrics'\n",
    "df['lyrics'] = df['lyrics'].apply(clean_lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test-Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['lyrics'], df['label'], test_size=0.2, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization:\n",
    "The Tokenizer API from TensorFlow Keras can split sentences into words and encode them into integers.\n",
    "\n",
    "The Tokenizer will perform all the necessary pre-processing steps:\n",
    "\n",
    "- tokenize into word character (word level)\n",
    "- num_words for maximum number of unique tokens\n",
    "- filter out the punctuation terms\n",
    "- convert all words to lower case\n",
    "- convert all words to integer index\n",
    "\n",
    "[Quelle](https://nzlul.medium.com/the-classification-of-text-messages-using-lstm-bi-lstm-and-gru-f79b207f90ad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing parameters\n",
    "max_len = 182 # Das 2 Quartil genommen von oben lyrics_count\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>' # out of vocabulary token\n",
    "vocab_size = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, \n",
    "                      char_level = False,\n",
    "                      oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_words : how many unique word that we want to load in training and testing data\n",
    "- oov_token : out of vocabulary token will be added to word index in the corpus which is used to build the model. This is used to replace out of vocabulary words (words that are not in our corpus) during text_to_sequence calls.\n",
    "\n",
    "[Quelle](https://nzlul.medium.com/the-classification-of-text-messages-using-lstm-bi-lstm-and-gru-f79b207f90ad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word_index\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index)\n",
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step : Let’s represent each sentence by sequences of numbers using texts_to_sequencesfrom Tokenizer object. After that, we padded the sequence so that we can have same length of each sequence.\n",
    "[Quelle](https://nzlul.medium.com/the-classification-of-text-messages-using-lstm-bi-lstm-and-gru-f79b207f90ad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traindata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "training_padded = pad_sequences(training_sequences,\n",
    "                                maxlen = max_len,\n",
    "                                padding = padding_type,\n",
    "                                truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "testing_padded = pad_sequences(testing_sequences,\n",
    "                               maxlen = max_len,\n",
    "                               padding = padding_type,\n",
    "                               truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- padding : ‘pre’ or ‘post (default pre). By using pre, we’ll pad before each sequence and post will pad after each sequence.\n",
    "- maxlen : maximum length of all sequences. If not provided, by default it will use the maximum length of the longest sentence.\n",
    "- truncating : ‘pre’ or ‘post’ (default ‘pre’). If a sequence length is larger than the provided maxlen value then, these values will be truncated to maxlen. ‘pre’ option will truncate at the beginning where as ‘post’ will truncate at the end of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of training tensor: ', training_padded.shape)\n",
    "print('Shape of testing tensor: ', testing_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Modell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter\n",
    "vocab_size = 500 \n",
    "embedding_dim = 16\n",
    "drop_value = 0.2\n",
    "n_dense = 24\n",
    "\n",
    "num_classes = df['label'].nunique() # Anzahl an Genres (für Softmax layer die Klassen)\n",
    "\n",
    "# Define Dense Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,\n",
    "                    embedding_dim))\n",
    "                    #,input_length = max_len''))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dropout(drop_value))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training des Modells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1500\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# One-hot encode labels damit es mit es mit der loss-Function funktioniert muss hotencoded werden\n",
    "y_train_encoded = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "history = model.fit(training_padded,\n",
    "                    y_train_encoded,\n",
    "                    epochs=num_epochs, \n",
    "                    validation_data=(testing_padded, y_test_encoded),\n",
    "                    #callbacks =[early_stop],\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(testing_padded, y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting der History:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dense_results = model.evaluate(training_padded, np.asarray(y_train_encoded), verbose=2, batch_size=256)\n",
    "valid_dense_results = model.evaluate(testing_padded, np.asarray(y_test_encoded), verbose=2, batch_size=256)\n",
    "print(f'Train accuracy: {train_dense_results[1]*100:0.2f}')\n",
    "print(f'Valid accuracy: {valid_dense_results[1]*100:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "- Test und Validation muss getrennt werden.\n",
    "- Modell muss optimiert werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Modell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter\n",
    "n_lstm = 128\n",
    "drop_lstm = 0.2\n",
    "# Define LSTM Model \n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, embedding_dim))\n",
    "model1.add(SpatialDropout1D(drop_lstm))\n",
    "model1.add(LSTM(n_lstm, return_sequences=False))\n",
    "model1.add(Dropout(drop_lstm))\n",
    "model1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model1.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs1 = 30\n",
    "early_stop1 = EarlyStopping(monitor='val_loss', patience=2)\n",
    "history1 = model1.fit(training_padded,\n",
    "                     y_train_encoded,\n",
    "                     epochs=num_epochs1, \n",
    "                     validation_data=(testing_padded, y_test_encoded),\n",
    "                     callbacks =[early_stop1],\n",
    "                     verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print der Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dense_results1 = model1.evaluate(training_padded, np.asarray(y_train_encoded), verbose=2, batch_size=256)\n",
    "valid_dense_results1 = model1.evaluate(testing_padded, np.asarray(y_test_encoded), verbose=2, batch_size=256)\n",
    "print(f'Train accuracy: {train_dense_results1[1]*100:0.2f}')\n",
    "print(f'Valid accuracy: {valid_dense_results1[1]*100:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "- Test und Validation muss getrennt werden.\n",
    "- Modell muss optimiert werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size,\n",
    "                     embedding_dim))\n",
    "model2.add(Bidirectional(LSTM(n_lstm,\n",
    "                              return_sequences = False)))\n",
    "model2.add(Dropout(drop_lstm))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "model2.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs2 = 30\n",
    "early_stop2 = EarlyStopping(monitor = 'val_loss',\n",
    "                           patience = 2)\n",
    "history2 = model2.fit(training_padded,\n",
    "                     y_train_encoded,\n",
    "                     epochs = num_epochs2,\n",
    "                     validation_data = (testing_padded, y_test_encoded),\n",
    "                     callbacks = [early_stop2],\n",
    "                     verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot History:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print der Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dense_results2 = model2.evaluate(training_padded, np.asarray(y_train_encoded), verbose=2, batch_size=256)\n",
    "valid_dense_results2 = model2.evaluate(testing_padded, np.asarray(y_test_encoded), verbose=2, batch_size=256)\n",
    "print(f'Train accuracy: {train_dense_results2[1]*100:0.2f}')\n",
    "print(f'Valid accuracy: {valid_dense_results2[1]*100:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(vocab_size,\n",
    "                     embedding_dim))\n",
    "model3.add(SpatialDropout1D(0.2))\n",
    "model3.add(GRU(128, return_sequences = False))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(num_classes, activation='softmax'))\n",
    "model3.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs3 = 30\n",
    "early_stop3 = EarlyStopping(monitor='val_loss', patience=2)\n",
    "history3 = model3.fit(training_padded,\n",
    "                     y_train_encoded,\n",
    "                     epochs=num_epochs3, \n",
    "                     validation_data=(testing_padded, y_test_encoded),\n",
    "                     callbacks =[early_stop3],\n",
    "                     verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot History:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print der Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dense_results3 = model3.evaluate(training_padded, np.asarray(y_train_encoded), verbose=2, batch_size=256)\n",
    "valid_dense_results3 = model3.evaluate(testing_padded, np.asarray(y_test_encoded), verbose=2, batch_size=256)\n",
    "print(f'Train accuracy: {train_dense_results3[1]*100:0.2f}')\n",
    "print(f'Valid accuracy: {valid_dense_results3[1]*100:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the four different models\n",
    "print(f\"Dense model loss and accuracy: {model.evaluate(testing_padded, y_test_encoded)} \\n------------------------------------------------------------\\n\" )\n",
    "print(f\"LSTM model loss and accuracy: {model1.evaluate(testing_padded, y_test_encoded)} \\n------------------------------------------------------------\\n\" )\n",
    "print(f\"Bi-LSTM model loss and accuracy: {model2.evaluate(testing_padded, y_test_encoded)} \\n------------------------------------------------------------\\n\" )\n",
    "print(f\"GRU model loss and accuracy: {model3.evaluate(testing_padded, y_test_encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen aus dem vergleich der Modelle dass die modelle wie folgt abgeschnitten haben:\n",
    "1. Platz: Dense\n",
    "2. Platz: GRU\n",
    "3. Platz: Bi-LSTM\n",
    "4. Platz: LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "- Verschiedene Songs labeln lassen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "2921c012e734ca0f770d4e96528c5dbc4a0ec169f61fe8f3b8bfe5cb5796d290"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
